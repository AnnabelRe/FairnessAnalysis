{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run different models (Baseline LGBM, Fair models: FGBM, FTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import metrics\n",
    "\n",
    "import skops.io as sio\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_percentage_error, roc_auc_score, auc, f1_score, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.metrics import make_scorer, ConfusionMatrixDisplay, PrecisionRecallDisplay    \n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, roc_curve, accuracy_score\n",
    "from fairgbm import FairGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run_Baseline = True\n",
    "Run_LGBM_nosen = False\n",
    "Run_FGBM = False\n",
    "\n",
    "model = \"fgbm\" #\"lgbm\" \"lgbm_nosen\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"xxx\" # add customized local output path\n",
    "input_path = \"xxx\" # add customized local input path\n",
    "\n",
    "y_train=pd.read_pickle(input_path+\"y_train.pkl\")\n",
    "y_test = pd.read_pickle(input_path+\"y_test.pkl\")                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_full = X_full[['VN_GESCHLECHT_1m_1.0', 'VN_GESCHLECHT_1m_2.0', 'VN_GESCHLECHT_1m_nan']].idxmax(axis=1).map({'VN_GESCHLECHT_1m_1.0': 0, 'VN_GESCHLECHT_1m_2.0': 1, 'VN_GESCHLECHT_1m_nan': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_gender = pd.read_pickle(input_path+\"S_gender.pkl\")\n",
    "S_gender_test = pd.read_pickle(input_path+\"S_gender_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_nat = pd.read_pickle(input_path+\"S_nat.pkl\")\n",
    "S_nat_test = pd.read_pickle(input_path+\"S_nat_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPC = False\n",
    "\n",
    "if PPC:\n",
    "    \n",
    "    X_train.reset_index(inplace=True)\n",
    "    X_test.reset_index(inplace=True)\n",
    "\n",
    "    S_gender= X_train[['VN_GESCHLECHT_1m_1.0', 'VN_GESCHLECHT_1m_2.0', 'VN_GESCHLECHT_1m_nan']].idxmax(axis=1).map({'VN_GESCHLECHT_1m_1.0': 0, 'VN_GESCHLECHT_1m_2.0': 1, 'VN_GESCHLECHT_1m_nan': 2})\n",
    "    S_gender_test= X_test[['VN_GESCHLECHT_1m_1.0', 'VN_GESCHLECHT_1m_2.0', 'VN_GESCHLECHT_1m_nan']].idxmax(axis=1).map({'VN_GESCHLECHT_1m_1.0': 0, 'VN_GESCHLECHT_1m_2.0': 1, 'VN_GESCHLECHT_1m_nan': 2})\n",
    "    nat_nan = ['VN_NATION_1m_None', 'VN_NATION_1m_<U>']\n",
    "    nat_other = X_train.columns[154:253].tolist()\n",
    "    sensitive_columns = nat_other + nat_nan + ['VN_NATION_1m_A' ,'VN_GESCHLECHT_1m_1.0', 'VN_GESCHLECHT_1m_2.0', 'VN_GESCHLECHT_1m_nan']\n",
    "    X_train_nats = X_train.iloc[:,154:253]\n",
    "    X_test_nats = X_test.iloc[:,154:253]\n",
    "\n",
    "    X_train[\"Nat_other\"] = np.logical_or.reduce(X_train_nats, axis = 1).astype(int)\n",
    "    X_train[\"Nat_nan\"] = np.logical_or(X_train[\"VN_NATION_1m_<U>\"],X_train[\"VN_NATION_1m_None\"]).astype(int)\n",
    "\n",
    "    X_test[\"Nat_other\"] = np.logical_or.reduce(X_test_nats, axis = 1).astype(int)\n",
    "    X_test[\"Nat_nan\"] = np.logical_or(X_test[\"VN_NATION_1m_<U>\"],X_test[\"VN_NATION_1m_None\"]).astype(int)\n",
    "    S_nat= X_train[['VN_NATION_1m_A', \"Nat_other\", \"Nat_nan\"]].idxmax(axis=1).map({'VN_NATION_1m_A': 0, \"Nat_other\": 1, \"Nat_nan\": 2})\n",
    "    S_nat_test= X_test[['VN_NATION_1m_A', \"Nat_other\", \"Nat_nan\"]].idxmax(axis=1).map({'VN_NATION_1m_A': 0, \"Nat_other\": 1, \"Nat_nan\": 2})\n",
    "\n",
    "    X_train.drop(columns=[\"Nat_other\",\"Nat_nan\"], inplace=True)\n",
    "    X_test.drop(columns=[\"Nat_other\",\"Nat_nan\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_nat_counts = S_nat.value_counts().sort_index()\n",
    "S_nat_test_counts = S_nat_test.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_train = S_nat_counts / len(S_nat)\n",
    "rel_test = S_nat_test_counts / len(S_nat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Austrian','Non-Austrian','Unknown']\n",
    "rel_test.index = index\n",
    "rel_train.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Create blue bar plots\n",
    "sns.barplot(x=rel_train.index, y=rel_train.values, ax=axes[0], color='C0')\n",
    "sns.barplot(x=rel_test.index, y=rel_test.values, ax=axes[1], color='C0')  # Fixed to use rel_test.values\n",
    "\n",
    "# Set titles\n",
    "fig.suptitle(\"Claims per Nationality\")\n",
    "axes[0].set_title(\"Training\")\n",
    "axes[1].set_title(\"Test\")\n",
    "\n",
    "# Format labels to 2 decimal places with percentage sign\n",
    "train_labels = [f\"{val:.2%}\" for val in rel_train]\n",
    "test_labels = [f\"{val:.2%}\" for val in rel_test]\n",
    "\n",
    "# Add formatted labels to bars\n",
    "axes[0].bar_label(axes[0].containers[0], labels=train_labels)\n",
    "axes[1].bar_label(axes[1].containers[0], labels=test_labels)\n",
    "\n",
    "# Set axis labels for each subplot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Nationality')\n",
    "    ax.set_ylabel('Relative Frequency')\n",
    "\n",
    "# Remove the redundant labels and title that were applied to the overall figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(output+\"EDA_Nat.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 0.8400000000000001,\n",
    " 'importance_type': 'split',\n",
    " 'learning_rate': 0.15100000000000002,\n",
    " 'max_depth': 5,\n",
    " 'min_child_samples': 4020,\n",
    " 'min_child_weight': 0.001,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': -1,\n",
    " 'num_leaves': 4,\n",
    " 'objective': 'binary',\n",
    " 'random_state': None,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'verbosity': -1,\n",
    " 'boost_from_average': False,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 3.0000000000000004,\n",
    " 'lambda_l2': 7.6,\n",
    " 'scale_pos_weight': 131}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Run_Baseline:\n",
    "    lgbm = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    y_pred_lgbm = lgbm.predict_proba(X_test)[:,1]\n",
    "    pd.DataFrame(y_pred_lgbm).to_pickle(input_path+\"y_pred_te_baseline.pkl\")\n",
    "    \n",
    "else: \n",
    "    y_pred_baseline  = pd.read_pickle(input_path+\"y_pred_te_baseline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = \"LGBM_baseline.skops\"\n",
    "unknown_types = sio.get_untrusted_types(file = model)\n",
    "clf=sio.load(model,trusted=unknown_types)\n",
    "\n",
    "test_data_pred = clf.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "y_pred_lgbm_nosen = clf.predict_proba(X_test)[:,1] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Run_Baseline:\n",
    "    feat_imp = pd.DataFrame(lgbm.feature_importances_, columns = ['imp'])\n",
    "    feat_imp['Label'] = X_train.columns\n",
    "    feat_imp_sign = feat_imp.sort_values(by = 'imp', ascending = False).head(20)\n",
    "    bow = feat_imp_sign.drop([1,6,8,0,14,12,20,4,16,3])\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "    ax.barh(bow['Label'].values, bow['imp'])\n",
    "    plt.title('Feature importances of LGBM Baseline Model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save = False\n",
    "\n",
    "if Save:\n",
    "    S_nat.to_pickle(\"S_nat.pkl\")\n",
    "    S_nat_test.to_pickle(\"S_nat_test.pkl\")\n",
    "    S_gender.to_pickle(\"S_gender.pkl\")\n",
    "    S_gender_test.to_pickle(\"S_gender_test.pkl\")\n",
    "    y_train.to_pickle(\"y_train.pkl\")\n",
    "    y_test.to_pickle(\"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Nosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_columns = nat_other + nat_nan + ['VN_NATION_1m_A' ,'VN_GESCHLECHT_1m_1.0', 'VN_GESCHLECHT_1m_2.0', 'VN_GESCHLECHT_1m_nan']\n",
    "\n",
    "X_train_nos =  X_train.drop(columns=sensitive_columns)\n",
    "X_test_nos =  X_test.drop(columns=sensitive_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Run_LGBM_nosen == True: \n",
    "\n",
    "    X_train= X_train_nos.copy()\n",
    "    X_test  = X_test_nos.copy()\n",
    "\n",
    "    lgbm = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    y_pred_lgbm_nosen = lgbm.predict_proba(X_test)[:,1]\n",
    "    pd.DataFrame(y_pred_lgbm).to_pickle(input_path+\"y_pred_lgbm_nosen.pkl\")\n",
    "    \n",
    "else: \n",
    "    y_pred_lgbm_nosen = pd.read_pickle(input_path+\"y_pred_lgbm_nosen.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairGBM - with and without HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'goss',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 0.8400000000000001,\n",
    " 'importance_type': 'split',\n",
    " 'is_unbalance' : True,\n",
    " 'learning_rate': 0.1,\n",
    " 'max_depth': -1,\n",
    " 'metric':'auc',\n",
    " 'min_child_samples': 4020,\n",
    " 'min_child_weight': 1,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': -1,\n",
    " 'num_leaves': 64,\n",
    " 'random_state': None,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'scale_pos_weight':1000,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'verbosity': -1,\n",
    " 'boost_from_average': False,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 3.0000000000000004,\n",
    " 'lambda_l2': 7.6,\n",
    " 'scale_pos_weight': 131}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE_N = int(0.5 * len(X_test))\n",
    "\n",
    "indices = np.random.permutation(len(X_test))\n",
    "val_indices = indices[VALIDATION_SIZE_N: ]\n",
    "test_indices = indices[: VALIDATION_SIZE_N]\n",
    "\n",
    "X_val = X_test.iloc[val_indices]\n",
    "X_test_fgbm = X_test.iloc[test_indices]\n",
    "\n",
    "S_gen_val = S_gender_test[val_indices]\n",
    "S_nat_val = S_nat_test[val_indices]\n",
    "\n",
    "S_gen_test_fgbm = S_gender_test[test_indices]\n",
    "S_nat_test_fgbm = S_nat_test[test_indices]\n",
    "\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "y_val = y_test[val_indices]\n",
    "y_test_fgbm = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hpt.tuner import ObjectiveFunction, OptunaTuner\n",
    " \n",
    "RN_HP_TUNING = True\n",
    " \n",
    "if RN_HP_TUNING:\n",
    "    HYPERPARAM_SPACE_PATH = (input_path+\"fgbm_params.yaml\")\n",
    " \n",
    "    obj_func = ObjectiveFunction(\n",
    "    X_train=X_train, y_train=y_train, s_train=S_gender,\n",
    "    X_val=X_val, y_val=y_val, s_val=S_gen_val,\n",
    "    hyperparameter_space=HYPERPARAM_SPACE_PATH,\n",
    "    eval_metric=\"accuracy\",\n",
    "    other_eval_metric=\"equalized_odds_ratio\",\n",
    "    threshold=0.50,\n",
    "    alpha=0.50)   # relative weight of `eval_metric` vs `other_eval_metric`\n",
    " \n",
    "    tuner = OptunaTuner(\n",
    "        objective_function=obj_func,\n",
    "        direction=\"maximize\",\n",
    "        seed=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Then just run optimize as you would for an optuna.Study object\n",
    "tuner.optimize(n_trials=20, n_jobs=max(2, os.cpu_count()), show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " obj_func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results.sort_values(by=\"equalized_odds_ratio\", ascending=False).head()[\n",
    "    ['accuracy', \"equalized_odds_ratio\", \"equalized_odds_diff\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best trial was #{obj_func.best_trial.id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fairgbm_clf = obj_func.reconstruct_model(obj_func.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Run_FGBM == True: \n",
    "    X_train= X_train_nos.copy()\n",
    "    X_test  = X_test_nos.copy()\n",
    "    \n",
    "    del params[\"objective\"]\n",
    "\n",
    "    # Gender\n",
    "    fgbm = FairGBMClassifier(constraint_type=\"FPR\",global_target_fnr=0.7,global_constraint_type=\"FNR\", **params)\n",
    "    fgbm.fit(X_train, y_train, constraint_group= S_gender)\n",
    "    y_pred_fgbm_gender = fgbm.predict_proba(X_test)[:, 1] \n",
    "    pd.DataFrame(y_pred_fgbm_gender).to_pickle(output+\"Predictions/y_pred_fgbm_gender.pkl\")\n",
    "\n",
    "    # Nationality\n",
    "    fgbm = FairGBMClassifier(constraint_type=\"FPR\",global_target_fnr=0.7,global_constraint_type=\"FNR\", **params)\n",
    "    # Train using features (X), labels (Y), and sensitive attributes (S)\n",
    "    fgbm.fit(X_train, y_train, constraint_group= S_nat)\n",
    "    y_pred_fgbm_nat = fgbm.predict_proba(X_test)[:, 1] \n",
    "    pd.DataFrame(y_pred_fgbm_nat).to_pickle(output+\"Predictions/y_pred_fgbm_nat.pkl\")\n",
    "\n",
    "else: \n",
    "    y_pred_fgbm_gender = pd.read_pickle(input_path+\"y_pred_fgbm_gender.pkl\")\n",
    "    y_pred_fgbm_gender = y_pred_fgbm_gender[0].to_numpy()\n",
    "\n",
    "    y_pred_fgbm_nat = pd.read_pickle(output+\"y_pred_fgbm_nat.pkl\")\n",
    "    y_pred_fgbm_nat = y_pred_fgbm_nat[0].to_numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
