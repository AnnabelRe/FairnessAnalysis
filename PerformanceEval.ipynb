{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f1fb56-45c1-4a0d-8fc8-19d9c7587ab9",
   "metadata": {},
   "source": [
    "# predictive performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a218d8-4dc2-4a9f-9cb6-2a90c41f63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import metrics_normalized\n",
    "import radar_chart\n",
    "import fairness_dashboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score,\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4cb51-130f-4bdc-a3ca-9bc2d2cf2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitive_attribute = \"nationality\"\n",
    "sensitive_attribute = \"gender\"\n",
    "\n",
    "input_path = \"xxx\" # add customized local path to input folder\n",
    "output_path = \"xxx\" # add customized local path to output folder\n",
    "\n",
    "y_true = pd.read_pickle(input_path+\"y_test.pkl\")\n",
    "\n",
    "baseline = pd.read_pickle(input_path+\"y_pred_te_baseline.pkl\")\n",
    "ftu = pd.read_pickle(input_path+\"y_pred_lgbm_nosen.pkl\")[0].to_numpy()\n",
    "#to = pd.read_pickle(input_path+\"ypred_to_nat_tprr_recall.pkl\")[0].to_numpy()\n",
    "\n",
    "if sensitive_attribute == 'gender':\n",
    "    fgbm = pd.read_pickle(input_path+\"y_pred_fgbm_gender.pkl\")[0].to_numpy()\n",
    "    hpt = pd.read_pickle(input_path+\"y_pred_fgbm_gen_hpt_v2.pkl\")[0].to_numpy()\n",
    "    #eodds = pd.read_csv(input_path+\"y_pred_eodds_gen_FU.csv\").to_numpy().flatten()\n",
    "    #roc = pd.read_csv(input_path+\"y_pred_roc_gen_FU.csv\")[\"0\"].to_numpy().flatten()\n",
    "    \n",
    "else: #nationality\n",
    "    fgbm = pd.read_pickle(input_path+\"y_pred_fgbm_nat.pkl\")[0].to_numpy()\n",
    "    hpt = pd.read_pickle(input_path+\"y_pred_nat_FGBM_HPT_model.pkl\")[0].to_numpy()\n",
    "    eodds = pd.read_csv(input_path+\"y_pred_eodds_nat_AU.csv\").to_numpy().flatten()\n",
    "    roc = pd.read_csv(input_path+\"y_pred_roc_nat_AU.csv\")[\"0\"].to_numpy().flatten()\n",
    "    \n",
    "\n",
    "# Generate different prediction sets with varying accuracy\n",
    "#all_pred = [baseline,ftu,fgbm,hpt,to,roc,eodds]\n",
    "all_pred = [baseline,ftu,fgbm,hpt]\n",
    "\n",
    "\n",
    "#model_names = [\"Baseline\", \"FTU\", \"FGBM\", \"FGBM with HPT\", \"ThresholdOpt\", \"ROC\", \"EOdds\"]\n",
    "model_names = [\"Baseline\", \"FTU\", \"FGBM\", \"FGBM with HPT\"]\n",
    "\n",
    "y_pred_list = []\n",
    "for pred in all_pred:\n",
    "    pred = (pred > 0.5).astype(int)\n",
    "    y_pred_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54f1ca-274b-4653-9967-043e7b838d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, y_pred_list, model_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models against a single ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred_list : list of array-like\n",
    "        List of predictions from different models.\n",
    "    model_names : list of str, optional\n",
    "        Names for the models. If None, will use Model_1, Model_2, etc.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with performance metrics for each model.\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = [f\"Model_{i+1}\" for i in range(len(y_pred_list))]\n",
    "    \n",
    "    if len(model_names) != len(y_pred_list):\n",
    "        raise ValueError(\"Number of model names must match number of prediction sets\")\n",
    "    \n",
    "    # Initialize dictionary to store results\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Balanced Accuracy': [],\n",
    "        'Recall': [],\n",
    "        'Precision': [],\n",
    "        'F1 Score': []\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics for each model\n",
    "    for name, y_pred in zip(model_names, y_pred_list):\n",
    "        results['Model'].append(name)\n",
    "        results['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "        results['Balanced Accuracy'].append(balanced_accuracy_score(y_true, y_pred))\n",
    "        results['Recall'].append(recall_score(y_true, y_pred))\n",
    "        results['Precision'].append(precision_score(y_true, y_pred))\n",
    "        results['F1 Score'].append(f1_score(y_true, y_pred))\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e43c8d-0631-4431-bb4f-4a8293080fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results_df = evaluate_models(y_true, y_pred_list, model_names)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n",
    "# results_df.to_csv('model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49305c1f-37a4-4fd3-9a73-e61af2c8a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = results_df\n",
    "# Set the style for all visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "def plot_facet_grid(df):\n",
    "    \"\"\"Create a facet grid of individual metrics with consistent colors for models and a legend\"\"\"\n",
    "    # Melt the DataFrame\n",
    "    df_melted = pd.melt(df, id_vars=['Model'], \n",
    "                         value_vars=['Accuracy', 'Balanced Accuracy', 'Recall', 'Precision', 'F1 Score'],\n",
    "                         var_name='Metric', value_name='Score')\n",
    "    \n",
    "    # Create a custom color palette for the models\n",
    "    unique_models = df['Model'].unique()\n",
    "    model_colors = dict(zip(unique_models, sns.color_palette(\"colorblind\", len(unique_models))))\n",
    "    \n",
    "    # Create a FacetGrid - single row with 5 columns and smaller height\n",
    "    g = sns.FacetGrid(df_melted, col='Metric', col_wrap=5, height=4, aspect=0.7, sharey=True)\n",
    "    \n",
    "    # Map the barplot with colors based on model and hide x-tick labels\n",
    "    def plot_colored_bars(data, **kwargs):\n",
    "        ax = sns.barplot(data=data, x='Model', y='Score', palette=model_colors, **kwargs)\n",
    "        \n",
    "        for patch in ax.patches:\n",
    "            current_width = patch.get_width()\n",
    "            patch.set_width(current_width * 0.7) \n",
    "            patch.set_x(patch.get_x() + current_width * 0.2)\n",
    "        \n",
    "        ax.set_xticklabels([])\n",
    "        # Add value labels on top of bars\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.2f', fontsize=7)\n",
    "    \n",
    "    g.map_dataframe(plot_colored_bars)\n",
    "    \n",
    "    g.set_titles(col_template=\"{col_name}\", fontsize=12 , pad=10)\n",
    "    g.set_axis_labels(\"\", \"Score\") \n",
    "    \n",
    "    g.set(ylim=(0, 1))\n",
    "    \n",
    "    # Create a custom legend (outside the facet grid)\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in model_colors.values()]\n",
    "    labels = list(model_colors.keys())\n",
    "    g.fig.legend(handles, labels, title=\"Model\", bbox_to_anchor=(1.02, 0.5), loc='center left')\n",
    "    plt.subplots_adjust(top=0.85, right=0.85)  \n",
    "    \n",
    "    #g.fig.suptitle('Performance Metrics by Model Type', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path+\"Perf_metrics_gen.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_facet_grid(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4993705-7284-43bb-b14b-7d1beae11565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
